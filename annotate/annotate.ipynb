{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotate documents for named entity recognition \n",
    "\n",
    "Select the document to tag, the output folder to save the annotations to, and a folder to save the model information to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document to tag (.txt file):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dac8ff933d84198bdd21b9d50377d25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileChooser(path='/home/abertsch/git/ADEPTLab/annotate', filename='', title='HTML(value='', layout=Layout(disp…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output folder to save annotations to:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ddb4a690fbe4c489249bfc4fed599ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileChooser(path='/home/abertsch/git/ADEPTLab/annotate', filename='', title='HTML(value='', layout=Layout(disp…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output folder to save model to:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a20a4e24b1b0465a88cb3b81377f08ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileChooser(path='/home/abertsch/git/ADEPTLab/annotate', filename='', title='HTML(value='', layout=Layout(disp…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from ipyfilechooser import FileChooser\n",
    "\n",
    "doc_loc = FileChooser()\n",
    "output_loc = FileChooser()\n",
    "model_loc = FileChooser()\n",
    "\n",
    "print(\"Document to tag (.txt file):\")\n",
    "display(doc_loc)\n",
    "print(\"Output folder to save annotations to:\")\n",
    "display(output_loc)\n",
    "print(\"Output folder to save model to:\")\n",
    "display(model_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the cell below to annotate the document! You can type a number (+enter) to label an entity, or just press enter to skip any words that are not entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAG OPTIONS: (press enter to leave untagged, b to go back)\n",
      "0 people, including fictional\t\t4 companies, institutions, etc.\n",
      "1 nationalities, religions\t\t5 countries, cities, states\n",
      "2 mountains, rivers, etc.\t\t6 events--named hurricanes, etc\n",
      "3 buildings, airports, etc.\t\t7 measurements (e.g. weight, distance)\n",
      "Department spokesperson Jan \u001b[32m <<Smith.>> \u001b[0m\n",
      "\tTAG? 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from colorama import Fore, Style\n",
    "from string import punctuation\n",
    "from IPython.display import clear_output\n",
    "\n",
    "fps = {}\n",
    "valid_inputs = {\"0\": \"PERSON\", \"1\": \"NORP\", \"2\": \"LOC\", \"3\": \"FAC\",\n",
    "                \"4\": \"ORG\", \"5\": \"GPE\", \"6\": \"EVENT\", \"7\": \"QUANTITY\"}\n",
    "\n",
    "stanford_core_tags = {\"PERSON\": \"PERSON\", \"NORP\": \"O\", \"LOC\": \"LOCATION\", \"FAC\": \"O\",\n",
    "                       \"ORG\": \"O\", \"GPE\": \"LOCATION\", \"EVENT\": \"O\", \"QUANTITY\": \"O\"}\n",
    "\n",
    "stanford_ann = \"\"\n",
    "spacy_ann = []\n",
    "fileout = \"\"\n",
    "pos = 0\n",
    "\n",
    "def print_tags():\n",
    "    print(\"TAG OPTIONS: (press enter to leave untagged)\")\n",
    "    print(\"0 people, including fictional\\t\\t4 companies, institutions, etc.\")\n",
    "    print(\"1 nationalities, religions\\t\\t5 countries, cities, states\")\n",
    "    print(\"2 mountains, rivers, etc.\\t\\t6 events--named hurricanes, etc\")\n",
    "    print(\"3 buildings, airports, etc.\\t\\t7 measurements (e.g. weight, distance)\")\n",
    "\n",
    "\n",
    "def annotate(fp):\n",
    "    file = fp.read()\n",
    "    words = file.split()\n",
    "    for i, word in enumerate(words):\n",
    "        get_tag(i, word, words)\n",
    "\n",
    "    fps[\"stanfordnlp-out\"].write(stanford_ann)\n",
    "    pickle.dump(spacy_ann, fps[\"spacy-out\"])\n",
    "    fps[\"rawtext-out\"].write(fileout)\n",
    "\n",
    "    for key in fps:\n",
    "        fps[key].close()\n",
    "\n",
    "\n",
    "\n",
    "def get_tag(i, word, words):\n",
    "    clear_output(wait=True)\n",
    "    print_tags()\n",
    "\n",
    "    for j in range(i - 3, i):\n",
    "        if j >= 0:\n",
    "            print(words[j] + \" \", end=\"\", flush=True)\n",
    "\n",
    "    print(f\"{Fore.GREEN} <<\" + word + f\">> {Style.RESET_ALL}\", end=\"\", flush=True)\n",
    "\n",
    "    for k in range(i + 1, i + 4):\n",
    "        if k < len(words):\n",
    "            print(\" \" + words[k], end=\"\", flush=True)\n",
    "\n",
    "    tag = input(\"\\n\\tTAG? \")\n",
    "    if tag == \"\" or tag in valid_inputs:\n",
    "        write_annotation(word, tag)\n",
    "\n",
    "    else:\n",
    "        print(f\"\\n{Fore.RED}Sorry, not sure what that meant. Try again.{Style.RESET_ALL}\", flush=True)\n",
    "        get_tag(i, word, words)\n",
    "\n",
    "    print()\n",
    "    return i\n",
    "\n",
    "def add_spacy_ann(word, tag):\n",
    "    word = word.strip(punctuation)\n",
    "    if(len(spacy_ann) != 0 and spacy_ann[-1][1] == pos and spacy_ann[-1][2] == valid_inputs[tag]):\n",
    "        spacy_ann.append((spacy_ann[-1][0], spacy_ann[-1][1] + 1 + len(word), valid_inputs[tag]))\n",
    "        spacy_ann.pop(-2)\n",
    "    else:\n",
    "        spacy_ann.append((pos + 1, pos + 1 + len(word), valid_inputs[tag]))\n",
    "\n",
    "\n",
    "def write_annotation(word, tag):\n",
    "    global fileout, stanford_ann, spacy_ann, pos\n",
    "\n",
    "    if tag == \"\":\n",
    "        fileout += \" \" + word\n",
    "        stanford_ann += word + \"\\t\" + \"O\" + \"\\n\"\n",
    "    else:\n",
    "        fileout += \" \" + word\n",
    "        stanford_ann += word + \"\\t\" + stanford_core_tags[valid_inputs[tag]] + \"\\n\"\n",
    "        add_spacy_ann(word, tag)\n",
    "\n",
    "    pos = pos + 1 + len(word)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    filename = doc_loc.selected\n",
    "    write_dir = output_loc.selected\n",
    "    print(doc_loc.selected)\n",
    "\n",
    "    fps[\"input\"] = open(filename)\n",
    "    fps[\"stanfordnlp-out\"] = open(os.path.join(write_dir, filename.split(\"/\")[-1].split(\".\")[0]+ \"-stanfordnlp.tsv\"), \"w+\")\n",
    "    fps[\"spacy-out\"] = open(os.path.join(write_dir, filename.split(\"/\")[-1].split(\".\")[0]+ \"-spacy.pkl\"), \"wb+\")\n",
    "    fps[\"rawtext-out\"] = open(os.path.join(write_dir, filename.split(\"/\")[-1].split(\".\")[0]+ \"-rawtext.txt\"), \"w+\")\n",
    "    annotate(open(filename))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs have been saved to the output folder specified, in formats suitable for training models with SpaCy or StanfordCore NLP. Run the cell below to train and output a new SpaCy model using that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(' The Alps are the most beautiful mountains in the world, according to Swiss National Tourism Department spokesperson Jan Smith.', [(5, 9, 'LOC'), (70, 103, 'ORG'), (117, 126, 'PERSON')])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(' The Alps are the most beautiful mountains in the world, according to Swiss National Tourism Department spokesperson Jan Smith.',\n",
       "  [(5, 9, 'LOC'), (70, 103, 'ORG'), (117, 126, 'PERSON')])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def get_data():\n",
    "    train_data = []\n",
    "    for file in os.listdir(output_loc.selected):\n",
    "        if(file.endswith('.txt')):\n",
    "            text = open(os.path.join(output_loc.selected, file)).read()\n",
    "            entities = pickle.load(open(os.path.join(output_loc.selected, file.rstrip('rawtext.txt') + 'spacy.pkl'), 'rb'))\n",
    "            train_data.append((text, entities))\n",
    "    print(train_data)\n",
    "    return train_data\n",
    "\n",
    "get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example-rawtext.txt\n",
      " The Alps are the most beautiful mountains in the world, according to Swiss National Tourism Department spokesperson Jan Smith.\n",
      "[(<_io.TextIOWrapper name='/home/abertsch/git/ADEPTLab/annotate/output/example-rawtext.txt' mode='r' encoding='UTF-8'>, '')]\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_lg'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-de77cfd6320b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Expected output:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-de77cfd6320b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(entities_loc, model, output_dir, n_iter)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;34m\"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en_core_web_lg\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# load existing spaCy model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loaded model '%s'\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_lg'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf8\n",
    "\"\"\"Example of training spaCy's named entity recognizer, starting off with an\n",
    "existing model or a blank model.\n",
    "\n",
    "For more details, see the documentation:\n",
    "* Training: https://spacy.io/usage/training\n",
    "* NER: https://spacy.io/usage/linguistic-features#named-entities\n",
    "\n",
    "Compatible with: spaCy v2.0.0+\n",
    "Last tested with: v2.2.4\n",
    "\"\"\"\n",
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "def get_data():\n",
    "    train_data = []\n",
    "    for file in os.listdir(output_loc.selected):\n",
    "        if(file.endswith('.txt')):\n",
    "            print(file)\n",
    "            text = open(os.path.join(output_loc.selected, file))\n",
    "            print(text.read())\n",
    "            entities = \"\" #pickle.load(open(os.path.join(output_loc.selected, file.rstrip('rawtext.txt') + 'spacy.pkl'), 'rb'))\n",
    "            train_data.append((text, entities))\n",
    "    print(train_data)\n",
    "    return train_data\n",
    "\n",
    "\n",
    "def main(entities_loc=None, model=None, output_dir=None, n_iter=100):\n",
    "    output_dir= model_loc.selected\n",
    "    TRAIN_DATA = get_data()\n",
    "\n",
    "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
    "    nlp = spacy.load(\"en_core_web_lg\")  # load existing spaCy model\n",
    "    print(\"Loaded model '%s'\" % model)\n",
    "\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    # otherwise, get it so we can add labels\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "    # add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    # only train NER\n",
    "    with nlp.disable_pipes(*other_pipes), warnings.catch_warnings():\n",
    "        # show warnings for misaligned entity spans once\n",
    "        warnings.filterwarnings(\"once\", category=UserWarning, module='spacy')\n",
    "\n",
    "        # reset and initialize the weights randomly – but only if we're\n",
    "        # training a new model\n",
    "        if model is None:\n",
    "            nlp.begin_training()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts,  # batch of texts\n",
    "                    annotations,  # batch of annotations\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    losses=losses,\n",
    "                )\n",
    "            print(\"Losses\", losses)\n",
    "\n",
    "    # test the trained model\n",
    "    for text, _ in TRAIN_DATA:\n",
    "        doc = nlp(text)\n",
    "        print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "        print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        for text, _ in TRAIN_DATA:\n",
    "            doc = nlp2(text)\n",
    "            print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "            print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "    # Expected output:\n",
    "    # Entities [('Shaka Khan', 'PERSON')]\n",
    "    # Tokens [('Who', '', 2), ('is', '', 2), ('Shaka', 'PERSON', 3),\n",
    "    # ('Khan', 'PERSON', 1), ('?', '', 2)]\n",
    "    # Entities [('London', 'LOC'), ('Berlin', 'LOC')]\n",
    "    # Tokens [('I', '', 2), ('like', '', 2), ('London', 'LOC', 3),\n",
    "    # ('and', '', 2), ('Berlin', 'LOC', 3), ('.', '', 2)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes the annotating and training notebook! Find the output annotations in the output folder and the model in the model folder that were selected in cell 1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
